---
title: "Neuromorphic engineering in 10 minutes"
date: 2023-08-31
description: "A brief take on neuromorphic computing and the technology involved."
draft: false
# image: framework-benchmarking-16k-header.png
tags: ["neuromorphic", "computing", "engineering", "overview"]
---

# Neuromorphic Engineering in 10 minutes
Neuromorphic engineering is a broad term that encompasses multiple approaches. Generally speaking, it takes inspiration from biological systems to process information as efficiently as possible. Such systems can be the mammal brain that is capable of language or abstract thought, a retina that compresses visual information or the navigation system of a bee. We then try to mimic those systems to a certain level of abstraction, by rebuilding them from the ground up. This brings is to the first important principle of neuromorphic engineering, which is that it relies on a new class of hardware. Why is that necessary? Because current hardware doesn’t work like biological systems! The von Neumann architecture that our computers are based on separates computation from memory and constantly has to move data around. This data movement costs up to 40% of the overall power budget, which is a huge waste! 

### In-memory computing

In a brain, the computation and data is co-located, meaning that the information is stored (in the form of ion concentrations, membrane potentials, synaptic connections) and processed (in the form of spike rates, phases or timings) in the same neuron! Therefore neuromorphic engineering is interested in what’s called in-memory computing, where data doesn’t have to be moved around but can be modified and used to calculate where it’s stored. Such in-memory computing exists in digital technology or more recently also in analog technology, where a new eletrical component called the memristor could bring potentially very high gains. Such memristors can be arranged on a large grid, which is called a crossbar array. Every node (crossing) in the array can be seen as a connection in a neural network. Then, instead of using digital CMOS technology and many transistors to represent numbers in a digital format, store and retrieve them from memory, add them together and then move all those bits again to storage, in the case of a crossbar array, we can work with eletrical currents and voltages directly! An input is therefore represented in the analog domain in the form of a current or voltage, in contrast to converting it to bits. This makes the computation extremely efficient, but also a bit more erroneous. That is an example of the trade-offs between computing in the analog versus in the digital domain. 

### Analog vs digital hardware

The analog circuit will necessarily have some variations between its components (resistors, transistors), and when used to compute directly, results will vary. The components of the digital circuit are subject to the same variations during fabrication (mainly because a transistor is now as wide as a few hundred atoms), but CMOS technology combines multiple of them to use digital representations (bits that are either 0 or 1) that are much more stable. There are two more principles that neuromorpic is making use of.

Analog hardware leverages continuous signals to mimic the behavior of biological neurons more closely. Its main strength lies in its ability to process information in parallel, providing high-speed and energy-efficient computations. Additionally, analog circuits are well-suited for solving complex, continuous-valued problems, making them ideal for tasks such as pattern recognition and sensory processing. However, analog hardware suffers from noise, limited precision, and calibration challenges, which can impact the accuracy and reliability of computations.

On the other hand, digital hardware employs discrete signals and binary logic, allowing for precise and reliable computations. Digital neuromorphic systems excel at handling spiking neural networks and tasks involving discrete values or events. The ability to perform complex mathematical operations accurately makes digital hardware suitable for tasks like complex mathematical modeling and data processing. Moreover, digital systems are inherently robust against noise and environmental variations, ensuring consistency in computations. However, they tend to consume more power and may face challenges in efficiently simulating certain biologically inspired neural behaviors.

### Asynchronous computation

Our current computer architecture is driven by clocks that time the exact executations of computations, ideally in parallel. That makes it possible to achieve really high throughput. The brain does not have such a central clock (or at least it hasn’t been discovered yet) and instead every neuron reacts in its own time to the arrival of input. The computation therefore happens asynchronously. When using neuromrophic backend processors, we ideally want to get our input data from neuromrophic sensors too. The most developed of such sensors is the neuromorphic camera, also known as event camera, dynamic vision sensor (DVS) or silicon retina. The event camera outputs data fully asynchronously, in contrast to a conventional camera that generates frames at a fixed rate. Its pixels act fully asynchronously and output changes in illumination rather than absolute light intensity values. That means that the output of the camera is now directly dependent on the activity in the scene. If no change is detected, then the camera doesn’t output anything and also the downstream neuromorpic processor doesn’t have to process anything! 

### Spike-based computation

There exist neural network accelerators that use analog technology to drive down the power cost (mythic.ai, rain.ai), some of which claim that this is enough to be considered neuromorphic. However when looking at how biology transmits information, action potentials, also called spikes, are a key factor. Neurons in our brain spontaneously spike all the time, and sometimes react to a certain input. All we know is that the brain can do amazing things by just using 20W of power. Try to match an equally-capable language model based on modern deep learning accelerators with that power budget, you will not even get close! A somewhat critical part of neuromorphic computing for some therefore is the computation using spikes. Most neuromorphic chips (IBM TrueNorth, Intel Loihi, Stanford’s Neurogrid, SpiNNaker, BrainScales, SynSense Speck) therefore implement a form of a spiking mechanism. When combining asynchronous and spiking computation for machine learning tasks, we end up with a new generation of neural networks called spiking neural networks.

### Spiking neural networks (SNN)

Spiking neural networks (SNNs) are bio-inspired computational models used in neuromorphic computing. Unlike traditional neural networks, SNNs communicate through discrete spikes, capturing temporal information for event-driven processing. They offer energy efficiency, real-time event detection, and pattern recognition capabilities. Challenges include specialized learning algorithms and ongoing efforts to improve training methods and hardware implementations. SNNs hold promise for efficient AI technologies in resource-constrained environments, advancing brain-inspired computing.