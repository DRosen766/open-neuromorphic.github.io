{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from expelliarmus import Wizard\n",
    "import pathlib\n",
    "import h5py\n",
    "import numpy as np\n",
    "import timeit\n",
    "import requests\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Download original data in evt3 raw format\n",
    "\n",
    "fname = \"driving_sample\"\n",
    "extension_map = {\n",
    "    'dat': 'dat',\n",
    "    'evt2': 'raw',\n",
    "    'evt3': 'raw',\n",
    "    'hdf5': 'hdf5',\n",
    "    'hdf5_lzf': 'hdf5',\n",
    "    'hdf5_gzip': 'hdf5',\n",
    "    'numpy': 'npy',\n",
    "}\n",
    "get_fpath = lambda encoding: f\"{fname}_{encoding}.{extension_map[encoding]}\"\n",
    "\n",
    "if not os.path.exists(get_fpath('evt3')):\n",
    "    # Downloading files.\n",
    "    print(\"Downloading EVT3 file... \", end=\"\")\n",
    "    if not pathlib.Path(get_fpath('evt3')).is_file():\n",
    "        r = requests.get(\"https://dataset.prophesee.ai/index.php/s/nVcLLdWAnNzrmII/download\", allow_redirects=True)\n",
    "        open(get_fpath('evt3'), 'wb').write(r.content)\n",
    "    print(\"done!\")\n",
    "\n",
    "wizard = Wizard(encoding=\"evt3\")\n",
    "data = wizard.read(get_fpath('evt3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Generate all comparison files\n",
    "\n",
    "# evt2 and dat\n",
    "raw_encodings = [\"dat\", \"evt2\", \"evt3\"]\n",
    "for encoding in raw_encodings:\n",
    "    if not os.path.exists(get_fpath(encoding)):\n",
    "        print(f\"Generating file for {encoding} encoding.\")\n",
    "        wizard = Wizard(encoding=\"evt3\")\n",
    "        wizard.set_encoding(encoding)\n",
    "        wizard.save(fpath=get_fpath(encoding), arr=data)\n",
    "\n",
    "# variants of hdf5\n",
    "hdf5_encodings = [\"hdf5\", \"hdf5_lzf\", \"hdf5_gzip\"]\n",
    "for encoding in hdf5_encodings:\n",
    "    fpath = pathlib.Path(f\"{fname}_{encoding}.hdf5\")\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Generating file for {encoding} encoding.\")\n",
    "        fp = h5py.File(fpath, \"w\")\n",
    "        fpath = pathlib.Path(get_fpath(encoding))\n",
    "        if encoding==\"hdf5\":\n",
    "            fp.create_dataset(name=\"events\", shape=data.shape, dtype=data.dtype, data=data)\n",
    "        elif encoding==\"hdf5_lzf\":\n",
    "            fp.create_dataset(name=\"events\", shape=data.shape, dtype=data.dtype, data=data, compression=\"lzf\")\n",
    "        elif encoding==\"hdf5_gzip\":\n",
    "            fp.create_dataset(name=\"events\", shape=data.shape, dtype=data.dtype, data=data, compression=\"gzip\")\n",
    "        fp.close()\n",
    "\n",
    "# numpy\n",
    "fpath = get_fpath('numpy')\n",
    "if not os.path.exists(fpath):\n",
    "    print(f\"Generating file for numpy encoding.\")\n",
    "    np.save(fpath, data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Run benchmarks\n",
    "\n",
    "REPEAT = 10\n",
    "get_fsize_MB = lambda fpath: round(fpath.stat().st_size/(1024*1024))\n",
    "\n",
    "# evt2, evt3, dat\n",
    "raw_times = []\n",
    "raw_sizes = []\n",
    "for encoding in raw_encodings:\n",
    "    fpath = get_fpath(encoding)\n",
    "    wizard = Wizard(encoding)\n",
    "    wizard.set_file(fpath)\n",
    "    raw_times.append(sum(timeit.repeat(lambda: wizard.read(fpath), number=1, repeat=REPEAT))/REPEAT)\n",
    "    raw_sizes.append(get_fsize_MB(pathlib.Path(fpath)))\n",
    "\n",
    "# hdf5 variants\n",
    "hdf5_times = []\n",
    "hdf5_sizes = []\n",
    "for encoding in hdf5_encodings:\n",
    "    fpath = get_fpath(encoding)\n",
    "    fp = h5py.File(fpath)\n",
    "    hdf5_times.append(sum(timeit.repeat(lambda: fp[\"events\"][:], number=1, repeat=REPEAT))/REPEAT)\n",
    "    fp.close()\n",
    "    hdf5_sizes.append(get_fsize_MB(pathlib.Path(fpath)))\n",
    "\n",
    "# numpy\n",
    "fpath = get_fpath('numpy')\n",
    "numpy_time = sum(timeit.repeat(lambda: np.load(fpath), number=1, repeat=REPEAT))/REPEAT\n",
    "numpy_size = get_fsize_MB(pathlib.Path(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Aggregate results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Encoding': raw_encodings + hdf5_encodings + [\"numpy\"],\n",
    "    'Framework': [\"expelliarmus\"] * len(raw_encodings) + [\"h5py\"] * len(hdf5_encodings) + [\"numpy\"],\n",
    "    'Read time [s]': raw_times + hdf5_times + [numpy_time],\n",
    "    'File size [MB]': raw_sizes + hdf5_sizes + [numpy_size],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Plot results\n",
    "\n",
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "\n",
    "title = f\"Reading the same {int(len(data)/1e6)} million events from different files.\"\n",
    "fig = px.scatter(df, x='Read time [s]', y='File size [MB]', color='Framework', symbol='Encoding', title=title)\n",
    "fig.update_traces(marker_size=13)\n",
    "fig.write_image('file_read_benchmark.png')\n",
    "# img_bytes = fig.to_image(format=\"png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "Only cells below here will be included in the article! To convert the notebook to markdown, run\n",
    "```\n",
    "jupyter nbconvert index.ipynb --to markdown --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_input_tags remove_input --TagRemovePreprocessor.remove_cell_tags remove_cell\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Reading events from disk, fast\"\n",
    "date: 2023-01-11\n",
    "description: \"Reduce loading times and disk footprint drastically. \"\n",
    "draft: true\n",
    "image: file_read_benchmark.png\n",
    "tags: [\"file encoding\", \"events\"]\n",
    "---\n",
    "In contrast to png/jpg for images, there is no standard format for events. When streaming data from an event camera, we get tuples of (time,x,y,polarity) that look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11718661,  762, 147, 1) (11718665,  833, 184, 1)\n",
      " (11718669, 1161,  72, 1) (11718674, 1110, 100, 0)\n",
      " (11718679, 1073,  23, 1) (11718684, 1134,  56, 1)\n",
      " (11718688,  799, 304, 0) (11718691,  391, 289, 0)\n",
      " (11718694,  234, 275, 1) (11718699,  512, 335, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(data[100:1100:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the emergence of event-based sensors, likewise came numerous ways of how to store the data. Some of the better ideas are hdf5 and numpy, some of the worse ones text files. When training spiking neural networks, file reading speed is a bottleneck we need to keep in mind. As the spatial resolution of event cameras grows, we receive more events for the same signal. Training on bigger datasets means that we want to keep in mind the file reading speed of our data. Here we list the results of our benchmark of different file type encodings and software frameworks that can decode files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![benchmark](file_read_benchmark.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file size depends on the encoding, whereas the reading speed depends on the particular implementation of how files are read. In terms of file size, we can see that numpy doesn't use any compression whatsoever, resulting in some 1.7GB file for our events. Prophesee's [evt3](https://docs.prophesee.ai/stable/data/encoding_formats/evt3.html) format achieves the best compression by cleverly encoding differences in timestamps. In terms of reading speed, numpy is the fastest as it doesn't deal with any compression on disk. Unzipping the events from disk on the other hand using h5py is by far the slowest. Using [Expelliarmus](https://github.com/open-neuromorphic/expelliarmus) and the [evt2](https://docs.prophesee.ai/stable/data/encoding_formats/evt2.html) file format, we get very close to numpy reading speeds while at the same time only using a fourth of the disk space. This becomes particularly important for larger datasets which can easily reach some 3-4TB because of inefficient file encodings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65b6f4b806bbaf5b54d6ccaa27abf7e5307b1f0e4411e9da36d5256169cebdd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
